# Monitoring & Alerting Rehberi

## Genel BakÄ±ÅŸ

AiTransaction sistemi, production-ready monitoring ve observability altyapÄ±sÄ± ile donatÄ±lmÄ±ÅŸtÄ±r. Bu guide, sistemi nasÄ±l monitoring yapacaÄŸÄ±nÄ±z, dashboards'Ä± nasÄ±l kullanacaÄŸÄ±nÄ±z ve alerts'u nasÄ±l ayarlayacaÄŸÄ±nÄ±zÄ± aÃ§Ä±klar.

## ğŸ“Š Monitoring Stack

### BileÅŸenler

| AraÃ§ | Port | AmaÃ§ | URL |
|------|------|------|-----|
| **Prometheus** | 9090 | Metrics collection & storage | http://localhost:9090 |
| **Grafana** | 3000 | Metrics visualization | http://localhost:3000 |
| **AlertManager** | 9093 | Alert routing & aggregation | http://localhost:9093 |
| **Elasticsearch** | 9200 | Log storage | http://localhost:9200 |
| **Kibana** | 5601 | Log visualization | http://localhost:5601 |

## ğŸš€ BaÅŸlangÄ±Ã§

### 1. Monitoring Stack'Ä± BaÅŸlat

```bash
# TÃ¼m servisleri baÅŸlat
docker-compose up -d

# Logs'u izle
docker-compose logs -f prometheus grafana alertmanager

# Health check
curl http://localhost:9090/-/healthy
curl http://localhost:3000/api/health
curl http://localhost:9093/-/healthy
```

### 2. Grafana'ya EriÅŸim

1. http://localhost:3000 aÃ§Ä±n
2. Login: `admin` / `admin`
3. Ä°lk login'de password deÄŸiÅŸmesi istenir (optional)
4. **Dashboards** â†’ **Browse** â†’ Pre-configured dashboards'Ä± gÃ¶receksiniz

### 3. Prometheus'a EriÅŸim

1. http://localhost:9090 aÃ§Ä±n
2. **Status** â†’ **Targets** kÄ±smÄ±nda tÃ¼m servisleri gÃ¶rebilirsiniz
3. SQL editor'da queries yazabilirsiniz

## ğŸ“ˆ Dashboards

### 01 - Overview Dashboard
**AmaÃ§:** Sistem health'inin genel durumunu gÃ¶rmek

**Key Metrics:**
- Total requests (req/sec)
- Error rate (%)
- Active services count
- Fraud detections (fraud/sec)
- Request rate timeline
- Error rate timeline
- Service health status table

**KullanÄ±m:**
- Sistem down mu? â†’ "Service Down" alert'ini kontrol edin
- Ne kadar trafik geÃ§iyor? â†’ "Total Requests" stat'Ä±nÄ± kontrol edin
- Fraud oranÄ± yÃ¼ksek mi? â†’ "Fraud Detections" stat'Ä±nÄ± kontrol edin

**Threshold Values:**
- âš ï¸ Error Rate > 5% = WARNING
- ğŸ”´ Error Rate > 10% = CRITICAL
- âš ï¸ Fraud Rate > 10% = INVESTIGATE

### 02 - API Performance Dashboard
**AmaÃ§:** API endpoint performance'Ä±nÄ± detaylÄ± olarak izlemek

**Key Metrics:**
- Request latency (p50, p95, p99)
- Requests by endpoint
- Errors by endpoint
- Response time distribution
- HTTP status code distribution
- Concurrent requests

**KullanÄ±m:**
- Slow API mi var? â†’ "Request Latency" grafiklerine bakÄ±n
- Hangi endpoint'te problem? â†’ "Errors by Endpoint" grafiklerine bakÄ±n
- Response time baseline nedir? â†’ "Response Time Distribution" grafiklerine bakÄ±n

**HealthCheck SLA:**
- p50: < 100ms âœ…
- p95: < 500ms âš ï¸ (>500ms = WARNING)
- p99: < 1s âš ï¸ (>1s = CRITICAL)

### 03 - Fraud Detection Dashboard
**AmaÃ§:** Fraud detection engine'inin performansÄ±nÄ± ve sonuÃ§larÄ±nÄ± izlemek

**Key Metrics:**
- Fraud detection rate (fraud/sec)
- Approval rate (approved/sec)
- Decision distribution (pie chart)
- Risk score histogram
- Fraud rules hit count
- Average processing time
- Success metrics

**KullanÄ±m:**
- Fraud detection dÃ¼n kaÃ§ adet? â†’ "Decisions Distribution" pie chart'Ä±na bakÄ±n
- Hangi rule en Ã§ok trigger? â†’ "Fraud Rules Hit Count" grafiklerine bakÄ±n
- Processing time normal mi? â†’ "Average Processing Time" grafiklerine bakÄ±n

**Normal Ranges:**
- Fraud Rate: 2-5% âœ…
- Fraud Rate > 10% = INVESTIGATE
- Processing Time: 50-200ms âœ…
- Processing Time > 500ms = SLOW

### 04 - System Resources Dashboard
**AmaÃ§:** Infrastructure health'Ä±nÄ± izlemek (DB, Cache, Message Queue, etc.)

**Key Metrics:**
- PostgreSQL connections
- Redis memory usage
- Redis cache hit rate
- Redis keys count
- Elasticsearch cluster health
- RabbitMQ memory usage
- Database query performance
- Elasticsearch document count
- RabbitMQ queue depths

**KullanÄ±m:**
- Database connection pool nearly full? â†’ "PostgreSQL Connections" (>180)
- Redis memory critical? â†’ "Redis Memory Usage" (>85%)
- Cache efficacy? â†’ "Redis Cache Hit Rate" (<70% = LOW)
- Message queue backed up? â†’ "RabbitMQ Queue Depths" (>1000 = HIGH)

**Resource Limits:**
- PostgreSQL connections: 200 max
- Redis memory: 512MB (docker-compose)
- Elasticsearch: 512MB heap
- RabbitMQ queue depth: < 1000 prefer

## ğŸš¨ Alerting

### Alert Types

#### 1. Critical Alerts (Immediate Action Required)
```
- Service Down (any service)
- High Error Rate (>5%)
- Database Connection Pool Exhausted (>180 connections)
- Fraud Worker Down
- Orchestrator Down
- Updater Down
```

**Custom Action:**
1. AlertManager notification alÄ±rsÄ±nÄ±z
2. Slack (if configured) veya Email
3. Grafana dashboard'a bakÄ±n
4. `docker-compose logs <service>` ile logs kontrol edin
5. Problem source'unu identify edin

#### 2. Warning Alerts (Monitor & Plan)
```
- High API Latency (p95 > 1s)
- High Memory Usage (>85%)
- Low Cache Hit Rate (<70%)
- High Queue Depth (>1000)
- High Fraud Detection Rate (>10%)
- Slow Queries
```

**Custom Action:**
1. Bir sonraki deployment cycle'Ä±nda optimize edin
2. Capacity planning reviewu yapÄ±n
3. Performance profiling consider edin

### Alert Configuration

**File:** `scripts/alertmanager.yml`

```yaml
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']

receivers:
  - name: 'critical-alerts'
    email_configs:
      - to: 'admin@example.com'  # CUSTOMIZE
    slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK'  # CUSTOMIZE
  
  - name: 'warning-alerts'
    slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK'  # CUSTOMIZE
```

### Slack Integration (Optional)

1. Slack workspace'te webhook'u create edin:
   - https://api.slack.com/apps â†’ Create App
   - Incoming Webhooks enable edin
   - Create Webhook URL (Ã¶r: https://hooks.slack.com/services/...)

2. `scripts/alertmanager.yml` gÃ¼ncelleyin:
```yaml
slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
    channel: '#critical-alerts'
```

3. Container'Ä± restart edin:
```bash
docker-compose restart alertmanager
```

## ğŸ“Š Custom Queries

### Prometheus Query Examples

**Request Rate (requests per second):**
```promql
sum(rate(http_server_request_duration_seconds_count[5m]))
```

**Error Rate (as percentage):**
```promql
sum(rate(http_server_request_duration_seconds_count{status=~"5.."}[5m])) 
/ 
sum(rate(http_server_request_duration_seconds_count[5m]))
```

**P95 Latency (milliseconds):**
```promql
histogram_quantile(0.95, rate(http_server_request_duration_seconds_bucket[5m])) * 1000
```

**Transaction Creation Rate:**
```promql
rate(transaction_created[5m])
```

**Fraud Detection Rate:**
```promql
rate(fraud_detected[5m])
```

**Cache Hit Ratio:**
```promql
rate(redis_keyspace_hits_total[5m]) 
/ 
(rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m]))
```

**Database Connection Pool Usage:**
```promql
pg_stat_activity_count / 200  # Shows as percentage
```

## ğŸ” Troubleshooting

### Problem: "No data" in Prometheus

**Causes:**
1. Metrics endpoint not exposed
2. Service down
3. Prometheus scrape config wrong

**Solution:**
```bash
# 1. Check if service is healthy
curl http://localhost:5000/health/live
# 2. Check if metrics endpoint exists
curl http://localhost:5000/metrics
# 3. Check Prometheus targets
# http://localhost:9090/targets â†’ look for RED status
# 4. Re-check docker-compose.yml prometheus service
```

### Problem: "Alerts not firing"

**Causes:**
1. Alert rules not loaded
2. AlertManager down
3. Wrong threshold values

**Solution:**
```bash
# 1. Check Prometheus alert status
# http://localhost:9090/alerts

# 2. Check if AlertManager is running
docker ps | grep alertmanager

# 3. Check AlertManager logs
docker-compose logs alertmanager

# 4. Reload AlertManager config
docker-compose restart alertmanager
```

### Problem: "Missing metrics for specific service"

**Causes:**
1. Service doesn't have OpenTelemetry enabled
2. Service port wrong in prometheus.yml
3. Service health check failing

**Solution:**
```bash
# 1. Verify service is healthy
curl http://service-name:port/health/live

# 2. Check if metrics endpoint exists
curl http://service-name:port/metrics

# 3. Check prometheus.yml for correct config:
#   - job_name: correct?
#   - targets: correct IP:port?
#   - metrics_path: /metrics?

# 4. Restart Prometheus
docker-compose restart prometheus
```

## ğŸ¯ Best Practices

### 1. Regular Dashboard Review
- GÃ¼nde en az 1 kez Overview dashboard'a bakÄ±n
- Weekly API Performance review yapÄ±n
- Anomalies investigate edin

### 2. Alert Tuning
- Alert thresholds'u business requirements'a gÃ¶re ayarlayÄ±n
- False positive'larÄ± minimize edin
- YÃ¼ksek severitysi olan alert'larÄ± prioritize edin

### 3. Metrics Retention
- Default: 30 days (docker-compose.yml)
- Production'da daha uzun period consider edin (90+ days)
- Grafana datasource'unda retention policy belirleyin

### 4. Performance Baselines
- Normal koÅŸullarda metric values'larÄ± kaydedin
- Seasonal variation'larÄ± identify edin
- Scaling decisions iÃ§in historical data kullanÄ±n

### 5. Security
- Grafana admin password'u deÄŸiÅŸtirin
- Prometheus'a authentication eklemek consider edin
- AlertManager credentials'Ä± secure tutun
- Logs'u regular basis'te review edin

## ğŸ“š Additional Resources

- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)
- [AlertManager Documentation](https://prometheus.io/docs/alerting/latest/overview/)
- [OpenTelemetry .NET](https://github.com/open-telemetry/opentelemetry-dotnet)
